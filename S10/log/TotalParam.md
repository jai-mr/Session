```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 48, 8, 8]           2,352
           Dropout-2             [-1, 49, 8, 8]               0
         LayerNorm-3             [-1, 49, 8, 8]           6,272
            Conv2d-4             [-1, 49, 8, 8]           2,450
            Conv2d-5             [-1, 49, 8, 8]           2,450
            Conv2d-6             [-1, 49, 8, 8]           2,450
           Softmax-7            [-1, 8, 49, 49]               0
            Conv2d-8             [-1, 49, 8, 8]           2,401
           Dropout-9             [-1, 49, 8, 8]               0
        Attention-10             [-1, 49, 8, 8]               0
          PreNorm-11             [-1, 49, 8, 8]               0
        LayerNorm-12             [-1, 49, 8, 8]           6,272
           Conv2d-13            [-1, 147, 8, 8]           7,203
             GELU-14            [-1, 147, 8, 8]               0
          Dropout-15            [-1, 147, 8, 8]               0
           Conv2d-16             [-1, 49, 8, 8]           7,203
          Dropout-17             [-1, 49, 8, 8]               0
      FeedForward-18             [-1, 49, 8, 8]               0
          PreNorm-19             [-1, 49, 8, 8]               0
        LayerNorm-20             [-1, 49, 8, 8]           6,272
           Conv2d-21             [-1, 49, 8, 8]           2,450
           Conv2d-22             [-1, 49, 8, 8]           2,450
           Conv2d-23             [-1, 49, 8, 8]           2,450
          Softmax-24            [-1, 8, 49, 49]               0
           Conv2d-25             [-1, 49, 8, 8]           2,401
          Dropout-26             [-1, 49, 8, 8]               0
        Attention-27             [-1, 49, 8, 8]               0
          PreNorm-28             [-1, 49, 8, 8]               0
        LayerNorm-29             [-1, 49, 8, 8]           6,272
           Conv2d-30            [-1, 147, 8, 8]           7,203
             GELU-31            [-1, 147, 8, 8]               0
          Dropout-32            [-1, 147, 8, 8]               0
           Conv2d-33             [-1, 49, 8, 8]           7,203
          Dropout-34             [-1, 49, 8, 8]               0
      FeedForward-35             [-1, 49, 8, 8]               0
          PreNorm-36             [-1, 49, 8, 8]               0
        LayerNorm-37             [-1, 49, 8, 8]           6,272
           Conv2d-38             [-1, 49, 8, 8]           2,450
           Conv2d-39             [-1, 49, 8, 8]           2,450
           Conv2d-40             [-1, 49, 8, 8]           2,450
          Softmax-41            [-1, 8, 49, 49]               0
           Conv2d-42             [-1, 49, 8, 8]           2,401
          Dropout-43             [-1, 49, 8, 8]               0
        Attention-44             [-1, 49, 8, 8]               0
          PreNorm-45             [-1, 49, 8, 8]               0
        LayerNorm-46             [-1, 49, 8, 8]           6,272
           Conv2d-47            [-1, 147, 8, 8]           7,203
             GELU-48            [-1, 147, 8, 8]               0
          Dropout-49            [-1, 147, 8, 8]               0
           Conv2d-50             [-1, 49, 8, 8]           7,203
          Dropout-51             [-1, 49, 8, 8]               0
      FeedForward-52             [-1, 49, 8, 8]               0
          PreNorm-53             [-1, 49, 8, 8]               0
        LayerNorm-54             [-1, 49, 8, 8]           6,272
           Conv2d-55             [-1, 49, 8, 8]           2,450
           Conv2d-56             [-1, 49, 8, 8]           2,450
           Conv2d-57             [-1, 49, 8, 8]           2,450
          Softmax-58            [-1, 8, 49, 49]               0
           Conv2d-59             [-1, 49, 8, 8]           2,401
          Dropout-60             [-1, 49, 8, 8]               0
        Attention-61             [-1, 49, 8, 8]               0
          PreNorm-62             [-1, 49, 8, 8]               0
        LayerNorm-63             [-1, 49, 8, 8]           6,272
           Conv2d-64            [-1, 147, 8, 8]           7,203
             GELU-65            [-1, 147, 8, 8]               0
          Dropout-66            [-1, 147, 8, 8]               0
           Conv2d-67             [-1, 49, 8, 8]           7,203
          Dropout-68             [-1, 49, 8, 8]               0
      FeedForward-69             [-1, 49, 8, 8]               0
          PreNorm-70             [-1, 49, 8, 8]               0
        LayerNorm-71             [-1, 49, 8, 8]           6,272
           Conv2d-72             [-1, 49, 8, 8]           2,450
           Conv2d-73             [-1, 49, 8, 8]           2,450
           Conv2d-74             [-1, 49, 8, 8]           2,450
          Softmax-75            [-1, 8, 49, 49]               0
           Conv2d-76             [-1, 49, 8, 8]           2,401
          Dropout-77             [-1, 49, 8, 8]               0
        Attention-78             [-1, 49, 8, 8]               0
          PreNorm-79             [-1, 49, 8, 8]               0
        LayerNorm-80             [-1, 49, 8, 8]           6,272
           Conv2d-81            [-1, 147, 8, 8]           7,203
             GELU-82            [-1, 147, 8, 8]               0
          Dropout-83            [-1, 147, 8, 8]               0
           Conv2d-84             [-1, 49, 8, 8]           7,203
          Dropout-85             [-1, 49, 8, 8]               0
      FeedForward-86             [-1, 49, 8, 8]               0
          PreNorm-87             [-1, 49, 8, 8]               0
        LayerNorm-88             [-1, 49, 8, 8]           6,272
           Conv2d-89             [-1, 49, 8, 8]           2,450
           Conv2d-90             [-1, 49, 8, 8]           2,450
           Conv2d-91             [-1, 49, 8, 8]           2,450
          Softmax-92            [-1, 8, 49, 49]               0
           Conv2d-93             [-1, 49, 8, 8]           2,401
          Dropout-94             [-1, 49, 8, 8]               0
        Attention-95             [-1, 49, 8, 8]               0
          PreNorm-96             [-1, 49, 8, 8]               0
        LayerNorm-97             [-1, 49, 8, 8]           6,272
           Conv2d-98            [-1, 147, 8, 8]           7,203
             GELU-99            [-1, 147, 8, 8]               0
         Dropout-100            [-1, 147, 8, 8]               0
          Conv2d-101             [-1, 49, 8, 8]           7,203
         Dropout-102             [-1, 49, 8, 8]               0
     FeedForward-103             [-1, 49, 8, 8]               0
         PreNorm-104             [-1, 49, 8, 8]               0
     Transformer-105             [-1, 49, 8, 8]               0
         Flatten-106               [-1, 49, 64]               0
        Identity-107                   [-1, 64]               0
       LayerNorm-108             [-1, 64, 1, 1]             128
          Conv2d-109             [-1, 10, 1, 1]             640
================================================================
Total params: 223,326
Trainable params: 223,326
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.13
Params size (MB): 0.85
Estimated Total Size (MB): 5.00
----------------------------------------------------------------
```
