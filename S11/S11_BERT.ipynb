{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg_zUA6TVmuz"
   },
   "source": [
    "\n",
    "**Student of EVA7 Batch awaiting EVA Phase II submitting EVA8 Transformer Assignments** </br>\n",
    "\n",
    "Repository github url : https://github.com/jai-mr/Session </br>\n",
    "\n",
    "Assignment Repository : https://github.com/jai-mr/Session/blob/main/S11/README.md</br>\n",
    "\n",
    "Submitted by : Jaideep R - No Partners</br>\n",
    "\n",
    "Registered email id : jaideepmr@gmail.com</br>\n",
    "\n",
    "\n",
    "**Objective: BERT Training code***</br>\n",
    "1. Collect custom dataset\n",
    "2. Perform noisy word prediction(swap any word 15% of times from a sentence with any other random word, and then predict the correct word)\n",
    "3. Share sample from dataset, training logs and 10 examples of input-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student of EVA7 Batch awaiting EVA Phase II submitting EVA8 Transformer Assignments** \n",
    "Repository github url : https://github.com/jai-mr/Session\n",
    "Assignment Repository : https://github.com/jai-mr/Session/blob/main/S11/README.md\n",
    "Submitted by : Jaideep R - No Partners\n",
    "Registered email id : jaideepmr@gmail.com\n",
    "\n",
    "**Objective:***</br>\n",
    "1. Collect custom dataset\n",
    "2. Perform noisy word prediction(swap any word 15% of times from a sentence with any other random word, and then predict the correct word)\n",
    "3. Share sample from dataset, training logs and 10 examples of input-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fc9a279"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5071,
     "status": "ok",
     "timestamp": 1679592577870,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "7f1d9b7e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import exists\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679592580504,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "Smr0_3OhYBZg"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib\n",
    "import torch\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25421,
     "status": "ok",
     "timestamp": 1679592608591,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "wqFcTVelWy0Z",
    "outputId": "4a953d22-04e6-49fc-a912-f17dca418c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\",force_remount=True)\n",
    "#gdrivepath=\"/content/gdrive/MyDrive/EVA8/s11/\"\n",
    "gdrivepath=\"E:\\\\DockerDesktop\\\\EVA8\\\\S11\\\\gd\\\\\"\n",
    "import os \n",
    "os.chdir(gdrivepath+\"s11/BERT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suEIaKqNXrOB"
   },
   "outputs": [],
   "source": [
    "#%cd /content/gdrive/My Drive/EVA8/s11/BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1679592612808,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "zefs_8t3XhoF",
    "outputId": "608bd4cc-62d9-45f0-81c6-0e464eb418fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\DockerDesktop\\\\EVA8\\\\S11\\\\gd\\\\s11\\\\BERT'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\DockerDesktop\\EVA8\\S11\\gd\\s11\\BERT\n"
     ]
    }
   ],
   "source": [
    "%cd BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679592616957,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "jYcSVPzeXz-b",
    "outputId": "1615c8cf-c658-42ff-fd8b-0c8b4fb962ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 17:30:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41e8b8ed"
   },
   "source": [
    "## Define Transformer BERT Model (Encoder Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679592621767,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "d344ec81"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad2513f5"
   },
   "source": [
    "## Dataset Creation/Handling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679592626428,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "4b080e03"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_switch=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        random_index = random.randint(0, (len(dataset.vocab)-1))\n",
    "        s = [(random_index, w) if random.random() < p_random_switch else (w, w) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9489a8da"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679592630656,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "88971379"
   },
   "outputs": [],
   "source": [
    "# Following Dataset Creation Code works only on Wikipedia, Use of other websites not tested\n",
    "\n",
    "## Extracts Paragraph Text and Links from webpage link input\n",
    "def extract_data(link, linkLimit = 2000, extractLinks = False):\n",
    "    \n",
    "    response = urllib.request.urlopen(link)\n",
    "\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    pageText = \"\"\n",
    "    \n",
    "    for content in soup.find_all('p'):\n",
    "        pageText += content.get_text()\n",
    "    \n",
    "    pageLinks = []\n",
    "    count = 0\n",
    "    if extractLinks:\n",
    "        print(\"Processing page links....\")\n",
    "        for subLink in soup.find_all('a', href=True):\n",
    "            if \"/wiki/\" in subLink['href']  and \":\" not in subLink['href'] and \"wikimedia\" not in subLink['href']:\n",
    "                pageLinks.append(\"https://en.wikipedia.org\" + subLink['href'])\n",
    "                count += 1\n",
    "            if count == linkLimit: ## Limit dataset collection to 2000 links\n",
    "                break\n",
    "    \n",
    "    return pageText, pageLinks\n",
    "    \n",
    "## Processes text to remove reference numbers, brackets and text within brackets\n",
    "def process_text(text):\n",
    "    \n",
    "    processedText = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        \n",
    "        if \":\" in line or len(line.strip()) <= 5: ## Ignores info captions or short lines\n",
    "            continue\n",
    "        else:\n",
    "            line = re.sub(\"\\(.*?\\)\", \"\", line) ## remove any text contained with and within brackets\n",
    "            line = re.sub(\"\\[.*?\\]\", \"\", line) ## remove reference numbers and square brackets\n",
    "            line = re.sub(\"\\\"\", \"\", line) ## remove quotes (can cause errors)\n",
    "            processedText += line + \"\\n\"\n",
    "    \n",
    "    return processedText\n",
    "\n",
    "## Main function that parses text from all wiki links found within startPage (including the text on startPage)\n",
    "def collect_dataset(startPage):\n",
    "    \n",
    "    ## Get Text and Links from start_page\n",
    "    startPageText, startPageLinks = extract_data(startPage, extractLinks = True)\n",
    "    \n",
    "    allText = \"\"\n",
    "    processedText = process_text(startPageText)\n",
    "    #print(processedText)\n",
    "    allText += processedText\n",
    "    #print(allText)\n",
    "    \n",
    "    pbar = tqdm(startPageLinks)\n",
    "    for i, link in enumerate(pbar):\n",
    "        pageText, _ = extract_data(link)\n",
    "        processedPageText = process_text(pageText)\n",
    "        allText += processedText\n",
    "    \n",
    "    return allText\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2f21b5f"
   },
   "source": [
    "## Collect Text Data for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167508,
     "status": "ok",
     "timestamp": 1679591088480,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "3ed41755",
    "outputId": "59508c19-7aa0-4f80-edf4-880678fec22b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/409 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page links....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 409/409 [07:11<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "## Creates dataset from Code defined in previous block\n",
    "## Text is scraped from the link input as well as all wiki links found within the linked page\n",
    "datasetText = collect_dataset(\"https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_mottos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679591136078,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "5rSBZQINbgvh"
   },
   "outputs": [],
   "source": [
    "## Save dataset to txt file\n",
    "with open('BERT_Dataset_file.txt', 'w') as f:\n",
    "    f.write(datasetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1679591159471,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "sEQeRiMhbqaC",
    "outputId": "40d25ad4-5e4b-4c63-bed0-9c04d9593eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_Dataset_file.txt\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 945543,
     "status": "ok",
     "timestamp": 1679592119303,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "WEBrzfskbY8O",
    "outputId": "7ec5d18a-151b-4717-8429-4142f67451b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page links....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [15:43<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "## Creates dataset from Code defined in previous block\n",
    "## Text is scraped from the link input as well as all wiki links found within the linked page\n",
    "datasetText = collect_dataset(\"https://en.wikipedia.org/wiki/United_States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1679592130992,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "14fd84a4"
   },
   "outputs": [],
   "source": [
    "## Save dataset to txt file\n",
    "with open('BERT_Dataset_file.txt', 'a') as f:\n",
    "    f.write(datasetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e470011"
   },
   "source": [
    "## Initialization & Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abcad24b",
    "outputId": "05c70d16-095c-457c-e26f-211e515ba1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n",
    "\n",
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'BERT_Dataset_file.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "#sentences = datasetText.lower().split(\"\\n\")\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab_bert_s11.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "056f913e"
   },
   "source": [
    "## Initialize Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679592446567,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "7061c77b",
    "outputId": "a3d1b2ce-8438-42f1-d75f-e72d00ce5de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "## CPU Device\n",
    "DATA_DIR='.'\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c00123df",
    "outputId": "056da027-60d2-42af-a7b7-bcf9f2b6ba0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "## Apple Silicon Metal Performance Shader\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679592519025,
     "user": {
      "displayName": "Jaideep R",
      "userId": "05457039333441489486"
     },
     "user_tz": -330
    },
    "id": "CNrfmwYXg6In"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad2e4d85"
   },
   "source": [
    "## Model Initialization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5414e7e7",
    "outputId": "7acf39d6-8d11-43b8-d91a-896666a906af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 8.15  | Δw: 3.968\n",
      "it: 10  | loss 7.21  | Δw: 2.904\n",
      "it: 20  | loss 6.71  | Δw: 2.797\n",
      "it: 30  | loss 6.3  | Δw: 2.955\n",
      "it: 40  | loss 5.92  | Δw: 2.9\n",
      "it: 50  | loss 5.6  | Δw: 2.79\n",
      "it: 60  | loss 5.34  | Δw: 2.683\n",
      "it: 70  | loss 5.08  | Δw: 2.622\n",
      "it: 80  | loss 4.9  | Δw: 2.585\n",
      "it: 90  | loss 4.68  | Δw: 2.55\n",
      "it: 100  | loss 4.53  | Δw: 2.523\n",
      "it: 110  | loss 4.32  | Δw: 2.489\n",
      "it: 120  | loss 4.18  | Δw: 2.459\n",
      "it: 130  | loss 4.02  | Δw: 2.444\n",
      "it: 140  | loss 3.86  | Δw: 2.398\n",
      "it: 150  | loss 3.74  | Δw: 2.376\n",
      "it: 160  | loss 3.6  | Δw: 2.338\n",
      "it: 170  | loss 3.48  | Δw: 2.292\n",
      "it: 180  | loss 3.36  | Δw: 2.29\n",
      "it: 190  | loss 3.27  | Δw: 2.275\n",
      "it: 200  | loss 3.11  | Δw: 2.235\n",
      "it: 210  | loss 3.06  | Δw: 2.208\n",
      "it: 220  | loss 2.94  | Δw: 2.195\n",
      "it: 230  | loss 2.83  | Δw: 2.131\n",
      "it: 240  | loss 2.75  | Δw: 2.119\n",
      "it: 250  | loss 2.66  | Δw: 2.098\n",
      "it: 260  | loss 2.61  | Δw: 2.094\n",
      "it: 270  | loss 2.46  | Δw: 2.021\n",
      "it: 280  | loss 2.42  | Δw: 2.024\n",
      "it: 290  | loss 2.33  | Δw: 1.974\n",
      "it: 300  | loss 2.26  | Δw: 1.954\n",
      "it: 310  | loss 2.2  | Δw: 1.907\n",
      "it: 320  | loss 2.15  | Δw: 1.905\n",
      "it: 330  | loss 2.05  | Δw: 1.867\n",
      "it: 340  | loss 2.03  | Δw: 1.843\n",
      "it: 350  | loss 1.89  | Δw: 1.796\n",
      "it: 360  | loss 1.88  | Δw: 1.772\n",
      "it: 370  | loss 1.82  | Δw: 1.763\n",
      "it: 380  | loss 1.79  | Δw: 1.757\n",
      "it: 390  | loss 1.74  | Δw: 1.731\n",
      "it: 400  | loss 1.71  | Δw: 1.702\n",
      "it: 410  | loss 1.68  | Δw: 1.684\n",
      "it: 420  | loss 1.61  | Δw: 1.675\n",
      "it: 430  | loss 1.58  | Δw: 1.648\n",
      "it: 440  | loss 1.56  | Δw: 1.652\n",
      "it: 450  | loss 1.47  | Δw: 1.6\n",
      "it: 460  | loss 1.44  | Δw: 1.608\n",
      "it: 470  | loss 1.42  | Δw: 1.595\n",
      "it: 480  | loss 1.39  | Δw: 1.591\n",
      "it: 490  | loss 1.38  | Δw: 1.606\n",
      "it: 500  | loss 1.34  | Δw: 1.565\n",
      "it: 510  | loss 1.31  | Δw: 1.554\n",
      "it: 520  | loss 1.28  | Δw: 1.55\n",
      "it: 530  | loss 1.27  | Δw: 1.536\n",
      "it: 540  | loss 1.25  | Δw: 1.541\n",
      "it: 550  | loss 1.19  | Δw: 1.484\n",
      "it: 560  | loss 1.23  | Δw: 1.514\n",
      "it: 570  | loss 1.19  | Δw: 1.468\n",
      "it: 580  | loss 1.15  | Δw: 1.45\n",
      "it: 590  | loss 1.17  | Δw: 1.451\n",
      "it: 600  | loss 1.14  | Δw: 1.41\n",
      "it: 610  | loss 1.11  | Δw: 1.382\n",
      "it: 620  | loss 1.11  | Δw: 1.404\n",
      "it: 630  | loss 1.1  | Δw: 1.374\n",
      "it: 640  | loss 1.11  | Δw: 1.386\n",
      "it: 650  | loss 1.09  | Δw: 1.325\n",
      "it: 660  | loss 1.04  | Δw: 1.328\n",
      "it: 670  | loss 1.04  | Δw: 1.307\n",
      "it: 680  | loss 1.03  | Δw: 1.289\n",
      "it: 690  | loss 1.02  | Δw: 1.286\n",
      "it: 700  | loss 0.97  | Δw: 1.238\n",
      "it: 710  | loss 0.98  | Δw: 1.263\n",
      "it: 720  | loss 1.01  | Δw: 1.237\n",
      "it: 730  | loss 0.98  | Δw: 1.246\n",
      "it: 740  | loss 0.99  | Δw: 1.23\n",
      "it: 750  | loss 0.96  | Δw: 1.211\n",
      "it: 760  | loss 0.94  | Δw: 1.245\n",
      "it: 770  | loss 0.95  | Δw: 1.213\n",
      "it: 780  | loss 0.94  | Δw: 1.194\n",
      "it: 790  | loss 0.92  | Δw: 1.191\n",
      "it: 800  | loss 0.9  | Δw: 1.196\n",
      "it: 810  | loss 0.92  | Δw: 1.183\n",
      "it: 820  | loss 0.9  | Δw: 1.201\n",
      "it: 830  | loss 0.9  | Δw: 1.19\n",
      "it: 840  | loss 0.91  | Δw: 1.19\n",
      "it: 850  | loss 0.89  | Δw: 1.181\n",
      "it: 860  | loss 0.88  | Δw: 1.163\n",
      "it: 870  | loss 0.88  | Δw: 1.181\n",
      "it: 880  | loss 0.85  | Δw: 1.139\n",
      "it: 890  | loss 0.84  | Δw: 1.143\n",
      "it: 900  | loss 0.84  | Δw: 1.15\n",
      "it: 910  | loss 0.83  | Δw: 1.139\n",
      "it: 920  | loss 0.84  | Δw: 1.167\n",
      "it: 930  | loss 0.85  | Δw: 1.17\n",
      "it: 940  | loss 0.83  | Δw: 1.159\n",
      "it: 950  | loss 0.82  | Δw: 1.154\n",
      "it: 960  | loss 0.81  | Δw: 1.156\n",
      "it: 970  | loss 0.81  | Δw: 1.139\n",
      "it: 980  | loss 0.8  | Δw: 1.166\n",
      "it: 990  | loss 0.82  | Δw: 1.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "#model = model.cuda()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 1000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    #masked_input = masked_input.cuda(non_blocking=True)\n",
    "    #masked_target = masked_target.cuda(non_blocking=True)\n",
    "    masked_input.to(DEVICE)\n",
    "    masked_target.to(DEVICE)\n",
    "    \n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss - original code\n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d8cec0c"
   },
   "source": [
    "## Sample Inputs & Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0694c82b"
   },
   "source": [
    "### Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7c27c42a",
    "outputId": "35be7ea2-f96e-408f-99ae-a55e965bedbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20, 3259])\n"
     ]
    }
   ],
   "source": [
    "batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "\n",
    "masked_input = batch['input']\n",
    "masked_target = batch['target']\n",
    "\n",
    "masked_input.to(DEVICE)\n",
    "\n",
    "output = model(masked_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39163112"
   },
   "source": [
    "### Convert Output to Token Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "14f72fc0",
    "outputId": "ae4d38b8-47e7-471e-c403-e4b48f84fa88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim = 2)\n",
    "probOutput = m(output)\n",
    "idxOutput = torch.argmax(probOutput, dim = 2)\n",
    "print(idxOutput.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0f1abc1"
   },
   "source": [
    "### Print Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4c8c1ae2",
    "outputId": "4e458071-9930-485d-faa2-aed48280c197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Example 0---------------\n",
      "Input: the american almost food almost , the world ' s largest , pioneered the drive - through format almost the\n",
      "Output: the american almost food , , the world ' s largest , pioneered the drive - through format in the\n",
      "------------Example 1---------------\n",
      "Input: it is generally accepted that the first inhabitants of he america migrated from siberia by he of the bering land\n",
      "Output: it is generally accepted that the first inhabitants of of america migrated from siberia by to of the bering land\n",
      "------------Example 2---------------\n",
      "Input: cabin football is by several measures the most popular spectator sport in cabin united cabin ; the national football league\n",
      "Output: the football is by several measures the most popular spectator sport in states united the ; the national football league\n",
      "------------Example 3---------------\n",
      "Input: religion in the united states 700 a 700 700 , about 64% of 700 700 the united states identified themselves\n",
      "Output: religion in the united states in a in united , about 64% of in in the united states identified themselves\n",
      "------------Example 4---------------\n",
      "Input: consumed were about 567 , 715 sheltered and unsheltered homeless persons in the u . s consumed in january 2019\n",
      "Output: the were about 567 , 715 sheltered and unsheltered homeless persons in the u . s . in january 2019\n",
      "------------Example 5---------------\n",
      "Input: it is generally accepted that the first inhabitants of north america migrated from siberia by way of the christians christians\n",
      "Output: it is generally accepted that the first inhabitants of north america migrated from siberia by way of the christians christians\n",
      "------------Example 6---------------\n",
      "Input: as of 2023 , the united states has the sixth highest documented incarceration rate and second largest prison population mexican\n",
      "Output: as of 2023 , the united states has the sixth highest documented incarceration rate and second largest prison population of\n",
      "------------Example 7---------------\n",
      "Input: emitter bordering the gulf of emitter are prone to hurricanes , and emitter of the world emitter s tornadoes occur\n",
      "Output: of bordering the gulf of , are prone to hurricanes , and of of the world , s tornadoes occur\n",
      "------------Example 8---------------\n",
      "Input: paleo - americans migrated from philip philip the north american mainland at least philip , 000 philip ago , and\n",
      "Output: paleo - americans migrated from the the the north american mainland at least of , 000 the ago , and\n",
      "------------Example 9---------------\n",
      "Input: hollywood , southeast northern southeast of los angeles , california , is southeast southeast in motion southeast production and the\n",
      "Output: hollywood , the northern the of los angeles , california , is the the in motion the production and the\n"
     ]
    }
   ],
   "source": [
    " \n",
    "for i in range(10):\n",
    "    inp = masked_input[i]\n",
    "    outp = idxOutput[i]\n",
    "    \n",
    "    print(f\"------------Example {i}---------------\")\n",
    "    inp_text = [dataset.rvocab[int(idx)] for idx in inp]\n",
    "    print(\"Input: \" + ' '.join(inp_text))\n",
    "    outp_text = [dataset.rvocab[int(idx)] for idx in outp]\n",
    "    print(\"Output: \" + ' '.join(outp_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a6fe390"
   },
   "source": [
    "## Single Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 3259])\n",
      "torch.Size([1, 20])\n",
      "tensor([[   5,    0,   72,    1,    3,    4,   43,    3, 1749,    3,  167,   21,\n",
      "          220,    4,  383,  127,  984,    9, 1750,    3]])\n",
      "Input: in the north , enlightenment and an enlightenment influx of immigrants from southern and eastern europe supplied a surplus enlightenment\n",
      "Output: in the north , of and an of influx of immigrants from southern and eastern europe supplied a surplus of\n"
     ]
    }
   ],
   "source": [
    "model_inp = masked_input[20]\n",
    "model_out = model(model_inp)\n",
    "print(model_out.shape)\n",
    "mid = m(model_out)\n",
    "idxOut_model = torch.argmax(mid, dim = 2)\n",
    "print(idxOut_model.shape)\n",
    "print(idxOut_model)\n",
    "\n",
    "inp_text = [dataset.rvocab[int(idx)] for idx in model_inp]\n",
    "print(\"Input: \" + ' '.join(inp_text))\n",
    "outp_text = [dataset.rvocab[int(idx2)] for idx2 in idxOut_model[0]]\n",
    "print(\"Output: \" + ' '.join(outp_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
