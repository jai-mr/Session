{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE9LjQPL9_h3"
      },
      "source": [
        "\n",
        "**Student of EVA7 Batch awaiting EVA Phase II submitting EVA8 Transformer Assignments** </br>\n",
        "\n",
        "Repository github url : https://github.com/jai-mr/Session </br>\n",
        "\n",
        "Assignment Repository : https://github.com/jai-mr/Session/blob/main/S11/README.md</br>\n",
        "\n",
        "Submitted by : Jaideep R - No Partners</br>\n",
        "\n",
        "Registered email id : jaideepmr@gmail.com</br>\n",
        "\n",
        "\n",
        "**Objective: Code for Sparse attention on GPT ***</br>\n",
        "1. Implement sparse attention in the GPT Code\n",
        "2. Train on custom data collected for training BERT in the first part of the assignment\n",
        "3. Share training logs and 10 examples of output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iuW1E3R9_h9",
        "outputId": "6e32c7c6-ce2f-4bc5-a0b0-a966b6c0067a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\",force_remount=True)\n",
        "gdrivepath=\"/content/gdrive/MyDrive/EVA8/s11/\"\n",
        "\n",
        "import os \n",
        "os.chdir(gdrivepath+'s11/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JozMQSR09_h_",
        "outputId": "462dd732-78af-406c-8b86-b5850069f09b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/EVA8/s11/s11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTeMCimC-czf",
        "outputId": "58aba91f-6493-46e9-c149-125db217d57a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBERT\u001b[0m/  model.py      ReadMe.md       S11_GPT.ipynb    utils.py\n",
            "\u001b[01;34mGPT\u001b[0m/   \u001b[01;34m__pycache__\u001b[0m/  S11_BERT.ipynb  S11_GPT_o.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(gdrivepath+'s11/')\n",
        "%pwd\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6T5f8wl-u2L",
        "outputId": "3e3d50b7-fc80-41df-f329-54d79af706e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBERT\u001b[0m/  model.py   S11_BERT.ipynb  S11_GPT_o.ipynb\n",
            "\u001b[01;34mGPT\u001b[0m/   ReadMe.md  S11_GPT.ipynb   utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elkPxvlhAEYs",
        "outputId": "1552960a-b222-4203-8e71-b48bd4248b4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir(gdrivepath+'s11/GPT/')\n",
        "%pwd\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHavg-CzNXDd",
        "outputId": "c816fb72-bb00-4f36-df79-c2367c362cf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBERT\u001b[0m/  model.py   S11_BERT.ipynb  S11_GPT_o.ipynb\n",
            "\u001b[01;34mGPT\u001b[0m/   ReadMe.md  S11_GPT.ipynb   utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YQNZekN9_iB"
      },
      "source": [
        "## Import Dataset, Model & Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qY3QSIT19_iB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer  # pip install transformers\n",
        "from model import Transformer\n",
        "\n",
        "from utils import (\n",
        "    BATCH_SIZE,\n",
        "    BLOCK_SIZE,\n",
        "    DEVICE,\n",
        "    DROPOUT,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EMBED,\n",
        "    NUM_HEAD,\n",
        "    NUM_LAYER,\n",
        "    MAX_ITER,\n",
        "    EVAL_INTER,\n",
        "    encode,\n",
        "    decode,\n",
        "    get_batch,\n",
        "    save_model_to_chekpoint,\n",
        "    estimate_loss,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-nI6QDGUPVIr",
        "outputId": "daa4258f-f8f1-4732-b156-4e0adbdc3782"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vibpxAMwMmN8",
        "outputId": "293f2f3d-ad22-4704-cfa5-0d3cdce9a6e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "%ls /content/gdrive/MyDrive/EVA8/s11/s11/GPT/GPT_Dataset_file.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N01D93ObAc6s",
        "outputId": "f53fec0c-4f58-48dd-fd47-b03cd685e3cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/EVA8/s11/s11/GPT/GPT_Dataset_file.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJkcNtZV9_iC",
        "outputId": "0f7877c4-6f74-42ac-f26b-253ddfdaa47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13562746 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# load model from checkpoint\n",
        "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
        "\n",
        "# example to decode sequence\n",
        "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
        "# max_new_tokens=20)[0].tolist()\n",
        "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
        "\n",
        "# raw data\n",
        "#path_do_data = \"data/english.txt\"\n",
        "path_do_data = \"/content/gdrive/MyDrive/EVA8/s11/s11/GPT/GPT_Dataset_file.txt\"\n",
        "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
        "# we use pretrained BERT tokenizer for performance improvements\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "# data_raw = data_raw[4000000:] # short dataset\n",
        "\n",
        "# train/val split\n",
        "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Sz7GGb9_iC",
        "outputId": "dbb4e0a4-e2df-4277-963f-23548a156795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with 89.48M parameters\n"
          ]
        }
      ],
      "source": [
        "# train a new model\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "# load model to GPU if available\n",
        "m = model.to(DEVICE)\n",
        "# print the number of parameters in the model\n",
        "print(\n",
        "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcPPvyGD9_iD"
      },
      "source": [
        "## Define Optimizer and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvFAyh7M9_iD",
        "outputId": "edf55e91-5227-4e74-ae2b-fffbd685c677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step          0 | train loss 10.8203 | val loss 10.8197\n",
            "step         50 | train loss 4.5970 | val loss 4.4879\n",
            "step        100 | train loss 3.2229 | val loss 3.2252\n",
            "step        150 | train loss 2.9052 | val loss 2.8946\n",
            "step        200 | train loss 2.8765 | val loss 2.7437\n",
            "step        250 | train loss 2.7470 | val loss 2.7874\n",
            "step        300 | train loss 2.7590 | val loss 2.7351\n",
            "step        350 | train loss 2.7120 | val loss 2.6929\n",
            "step        400 | train loss 2.6833 | val loss 2.6397\n",
            "step        450 | train loss 2.6882 | val loss 2.6933\n",
            "step        499 | train loss 2.6578 | val loss 2.6486\n"
          ]
        }
      ],
      "source": [
        "# optimizer takes the model's parameters and the learning rate as input,\n",
        "# and updates the parameters during the training process in order to\n",
        "# minimize the loss function.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
        "MAX_ITER = 500\n",
        "EVAL_INTER = 50\n",
        "for step in range(MAX_ITER):\n",
        "\n",
        "    # every EVAL_INTER evaluate the loss on train and val sets\n",
        "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
        "        loss_train = estimate_loss(\n",
        "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        loss_val = estimate_loss(\n",
        "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward() method on the loss variable calculates the gradients \n",
        "    # of the loss with respect to the model's parameters.\n",
        "    loss.backward()\n",
        "    # step() method on the optimizer updates the model's parameters \n",
        "    # using the calculated gradients, in order to minimize the loss.\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVuzwliY9_iE"
      },
      "source": [
        "## Save Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi7qpIEw9_iF",
        "outputId": "05dec358-3dfe-48b9-8b1b-cc9a8c0e4e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved the model to checkpoint/checkpoint_epoch-499_24.03.2023_09:37:06.pt\n"
          ]
        }
      ],
      "source": [
        "save_model_to_chekpoint(model = m, path_to_checkpoint=\"checkpoint\", epoch=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvz8goal9_iG"
      },
      "source": [
        "## Generate Output Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPXSzJx09_iG"
      },
      "outputs": [],
      "source": [
        "# generate some output based on the context\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"-------------Output Example {i}----------------\")\n",
        "    print(\n",
        "        decode(\n",
        "            enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaV4f-aO9_iG",
        "outputId": "dee0df11-f664-43c5-a7db-022c7508b059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Output Example 1----------------\n",
            "[CLS] this is the sound of music [SEP] average. the second - gallup, giving vast quantities of government that number of the first wave of the united states holds jurisdiction that the four territories. 6 have won the five\n",
            "-------------Output Example 2----------------\n",
            "[CLS] do re me fa so la ti do. [SEP] caricaid. on their diet by france and the british army to western migration to be the country. military and the creation of world war ii, or their prison policy accounts\n",
            "-------------Output Example 3----------------\n",
            "[CLS] do a deer a female deer. [SEP] a situation that does not ratify the second term in the communist forces of those reside. of reconstruction began the second british army at the u. citizens and measures to the 1846\n",
            "-------------Output Example 4----------------\n",
            "[CLS] ray a drop of golden sun. [SEP] the three - highest total - de facto national guard brought the closest ally of 2020. the soviet union as of became increasingly rare, and sheriff. as the pacific coast guard brought\n",
            "-------------Output Example 5----------------\n",
            "[CLS] me a name i call myself. [SEP] is over a series of its military, chef james beard hosted the global audiences. s. s. the world, neck pain, the country joined the mississippian, and the\n",
            "-------------Output Example 6----------------\n",
            "[CLS] let us move sentence six. [SEP] for impoverished, 84, 6 % of the army, the united states. the constitutionalyzed by the americas. it is one of north america, ernest hemingway, and racial\n",
            "-------------Output Example 7----------------\n",
            "[CLS] fa a long long way to run. [SEP] the wealthiest 1, formally expanding across all nations host the allies on poverty expanded internal to the cold justice of life times the 1940's. ranked 2nd in the 1914 until the\n",
            "-------------Output Example 8----------------\n",
            "[CLS] do ti la so fa me re do. [SEP] income workers paid family leave school year of the wealthiest 2. ranks first documented arrival of americans's denali is occupied by direct vote count that does not carry health care outcomes\n",
            "-------------Output Example 9----------------\n",
            "[CLS] when i feel like i am alone. [SEP] there are the thirteenth amendment of the evidence suggests an alpine climate change inequalities, while most patents granted be the second samoa, to the global innovation since the 1803 by\n",
            "-------------Output Example 10----------------\n",
            "[CLS] i sing this song of my favourite things. [SEP] cause of the number of the french, and millionaires ; or six until the sitting president elected at least 12, high blood sugar, the first nation, native population and lied\n"
          ]
        }
      ],
      "source": [
        "# generate 10 input sentences\n",
        "sentences = ['This is the sound of Music',\n",
        "             'Do Re me fa so la ti do.',\n",
        "             'Do a deer a female deer.',\n",
        "             'Ray a drop of golden sun.',\n",
        "             'Me a name I call myself.',\n",
        "             'Let us move sentence six.',\n",
        "             'Fa a long long way to run.',\n",
        "             'do ti la so fa me re do.',\n",
        "             'When I feel like I am alone.',\n",
        "             'I sing this song of my favourite things.']\n",
        "\n",
        "# tokenize the input sentences\n",
        "tokenized_sentences = [tokenizer.encode(sentence) for sentence in sentences]\n",
        "i=1\n",
        "# generate output for each sentence\n",
        "for sentence in tokenized_sentences:\n",
        "    print(f\"-------------Output Example {i}----------------\")\n",
        "    context = torch.tensor(sentence, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "    output_tokens = m.generate(idx=context, max_new_tokens=35, block_size=BLOCK_SIZE)[0]\n",
        "    output_sentence = decode(enc_sec=output_tokens, tokenizer=tokenizer)\n",
        "    print(output_sentence)\n",
        "    i=i+1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUnrXq7GBJiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}